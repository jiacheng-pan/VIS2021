\section{Results and Evaluation}

We implement our system in a browser-based architecture.
The front-end application is developed with JavaScript using React and D3. The back-end server uses Python 3.7.5 with flask, networkx, numpy, and scipy. All experiments are performed on a Macbook Pro laptop with an Intel Core i7-7820HQ CPU (2.9 GHz) and 16 GiB RAM. 

\begin{figure}[t!p]
    \centering
    \setlength{\belowcaptionskip}{-5pt}
    \includegraphics[width=1\columnwidth]{picture/quancomp}
    \caption{Quantitative comparison of several conventional graph matching methods and our approach: (a) average accuracy of different frame spacing in the CMU-house-image dataset; (b) average accuracy of different numbers of outliers in the Motorbike-image dataset; (c) average accuracy of different numbers of outliers in the Car-image dataset.}
    \label{fig:quancomp}
\end{figure} 


\begin{figure*}[htb]
    \centering
    \setlength{\belowcaptionskip}{-5pt}
    \includegraphics[width=2\columnwidth]{picture/powernetwork}
    \caption{Case study with Power-Network dataset~\cite{nr}: (a) \added{an exemplar (in red) and two retrieved substructures (in blue, which are overlapped) overlaid on a network depicted using FM$^3$~\cite{hachul2004drawing}.} \added[id=pan]{(b) Two retrieved substructures are discarded. And several target substructures are specified manually.}(c) \replaced[id=pan]{The shape of the exemplar}{An exemplar is specified and its shape} is changed to a circle. \deleted[id=pan]{Several target substructures and their markers are specified.}  Modification transfer alters the node positions of targets to simulate the exemplar's shape. (d) All modified substructures are merged into the graph by an automatic optimization. \added{(e-h) Readability before (orange) and after (purple) modification transfer measured by four readability criteria (from top to bottom: crosslessness, minimum-angle metric, edge-length variant, and shape-based metric);; error bars depict 95\% confidence intervals.}}
    \label{fig:powernetwork}
\end{figure*}

\subsection {Quantitative Comparison} \label{sec:quancomp}
Our approach 
%leverages 
uses
a set of markers generated by graph-matching methods for modification transfer. 
% The result of graph matching can be used as markers. 
% \deleted{In this study,}
We compared conventional graph-matching methods to ours
% \deleted{as described in Section~\ref{sec:tuning}.}
using the following benchmark datasets \added[id=pan]{with manually labeled ground truth}:

\begin{compactenum}[\bfseries 1)]

\item \textbf{The CMU-house-image dataset}~\cite{zhou2012factorized} %  \deleted{is commonly used to test graph matching performance. The dataset} 
contains 111 frames of a house with 30 landmarks. We randomly remove 5 landmarks and generate a graph with Delaunay triangulation that connects landmarks for each frame. Frames are paired spaced by 0, 25, 50, and 75 frames, yielding 444 pairs.

\item \textbf{The Car-and-Motorbike image dataset}~\cite{DBLP:journals/ijcv/LeordeanuSH12} has 30 pairs of car images and 20 pairs of motorbike images. % \deleted{are contained in the dataset.} 
We used Delaunay triangulation to generate graphs for each image, added 0, 4, 8, 12, 16, and 20 outliers randomly, and removed unconnected nodes, yielding 222 pairs of graphs.
    %  Delaunay triangulation is employed to build graphs. We generate 6 graphs for each image by adding 0, 4, 8, 12, 16, and 20 outliers. Unconnected graphs are removed, yielding 222 pairs of graphs.

\end{compactenum}

Node-link diagrams of these datasets are generated by the well-studied force-directed layout algorithm. We compare the accuracy of graph matching results. Figure~\ref{fig:quancomp} shows the average matching accuracy on different datasets. Our approach works slightly better than FGMU and exceeds other methods, meaning that our improvements on FGMU can generate more accurate results in most cases. \added[id=pan]{Note that graphs in these benchmark datasets are smaller than those in the case studies.}


\subsection{Case Studies}
We show how our exemplar-based layout fine-tuning approach works in three case studies.

We used FM$^3$~\cite{hachul2004drawing} to generate the layout of the \textbf{Finan512 dataset} from the University of Florida Sparse Matrix Collection~\cite{10.1145/2049662.2049663}, which is generated from multistage stochastic financial modeling~\cite{DBLP:journals/jgo/SoperWC04}. 
The graph consisted of 74,752 nodes and 261,120 edges rendered in WebGL 
% \deleted{FM$^3$ is employed for generating its layout, and WebGL is utilized for visualization} 
(Figure~\ref{fig:Finan512}a). 
We saw several ``donut-like" substructures.

Next, we specified a substructure (here called an \textit{exemplar}) for fine-tuning (Figure~\ref{fig:Finan512}b). 
%Utilizing the pre-computed GraphWave-based~\cite{DBLP:conf/kdd/DonnatZHL18} representation, 
We retrieved similar substructures %are retrieved 
%by setting 
using
$k = 200$, $min=10$, $max=100$, and $\epsilon=0.95$ (Figure~\ref{fig:Finan512}a). 
\deleted{We find that these substructures are potentially isomorphic (though because their overall layout quality is poor, it is hard to identify whether they are isomorphic or similar).} To verify the topology of these substructures, we select 
%as
target substructures as the five most similar and five most dissimilar substructures according to their Weisfeiler-Lehman similarities to the exemplar (Figure~\ref{fig:Finan512}c).
In addition, to fine-tune the ``donut" subgraph, we use substructures around it 
%are specified 
as target substructures.

We interactively modified the exemplar into a layout with a distinguishable structure (Figure~\ref{fig:Finan512}e).
After modification transfer, these substructures became clearer (Figures~\ref{fig:Finan512}(f, g)). 
\deleted{Most of them are isomorphic. Nodes in several non-isomorphic but similar structures are placed in harmony with the exemplar. 
}

Our smooth merging scheme generated 
visually pleasing details compared to direct merging without any optimization. For example, the 
%borders
boundary
of the substructure in Figure~\ref{fig:finan512detail}c is easier to distinguish than the one without optimization in Figure~\ref{fig:finan512detail}b.

\textbf{The Power-Network dataset} is collected from the Network Data Repository~\cite{nr}, which abstracts a power system: the nodes encode buses and edges are the transmission lines among the nodes. The network contains 662 nodes and 906 edges. 
A multilevel graph layout implemented by Tulip~\cite{auber2004tulip} and \added[id=pan]{OGDF~\cite{DBLP:reference/crc/ChimaniGJKKM13}} is employed to layout the network (Figure~\ref{fig:powernetwork}a).

To reveal transmissions among a set of buses that may form a cycle\deleted[id=pan]{, nodes are formed into a circle shape.}, we specify a set of nodes as an exemplar (Figure~\ref{fig:powernetwork}a, in red).\deleted[id=pan]{The exemplar is interactively modified into a circle.} \replaced[id=pan]{With $min=10$, $max=100$, $k=5$, and $\epsilon=0.5$, two overlapped structures are retrieved (Figure~\ref{fig:powernetwork}a, in blue). The retrieved structures are not topologically similar to the exemplar, because our technique detects embedding-similar structures, which are potentially similar to the exemplar.
% 找拓扑相似的结构特别困难，所以我们搜索了embedding相似的结构。它们往往能有相似的拓扑结构，但它的精度依赖于embedding本身。
}{The retrieved similar structures are not satisfactory.}
Thus we explore the node-link diagram to specify target substructures. Several sets of nodes that may form cycles are specified as target substructures (Figure~\ref{fig:powernetwork}b, in blue). Connections among these nodes are obscured by the visual clutter. \added[id=pan]{The exemplar is interactively modified into a circle.} Each target is deformed into a circle-like shape by transferring modifications (Figure~\ref{fig:powernetwork}c). Now the connections among nodes are far more distinguishable (Figure~\ref{fig:powernetwork}d) than the original layout.

We increase $\alpha$ to increase the degree of orientation preservation, which means that orientations of edges tend to remain unchanged. This makes the shape of the modified target substructure smoother (Figure~\ref{fig:pownetweight}c).
Because the edge lengths before modification transfer are not identical (Figure~\ref{fig:pownetweight}a), solely preserving distances can lead to unsatisfying deformations. For example, setting $\alpha$ in Equation~\ref{eq:smoothness} to be zero generates irregular polygons (Figure~\ref{fig:pownetweight}b).

\begin{figure}[!tp]
    \centering
    \setlength{\belowcaptionskip}{-5pt}
    \includegraphics[width=1\columnwidth]{picture/pownetweight}
    \caption{Different weighting schemes for the Power-Network dataset. (a) a target substructure; (b) a low preservation on orientations with $\alpha = 0$, $\beta = 1$, and $\gamma = 100$; (c) a large preservation on orientations with $\alpha = 20$, $\beta = 1$, and $\gamma = 100$.}
    \label{fig:pownetweight}
\end{figure}


\textbf{The Price\textunderscore1000 dataset} is a tree\deleted[id=pan]{-like graph} from tsNET~\cite{DBLP:journals/cgf/KruigerRMKKT17} that consists of 1000 nodes and 999 edges. We layout the graph with a simple radial tree layout algorithm~\cite{DBLP:conf/infovis/Jankun-KellyM03} (Figure~\ref{fig:price1000}a), and find that sibling nodes are overlapped due to the space constraint. 

We select one representative subtree as an exemplar. To reduce visual clutter,this is reconfigured into a radial tree layout (Figure~\ref{fig:price1000}b). To reconfigure other interested subtrees, we specify two nodes of the exemplar as markers, and the algorithm transfers modifications on the exemplar to other subtrees (Figure~\ref{fig:price1000}c).

Although there are some unpleasing details, their layouts are similar to the exemplar's. Rather than interactively reconfiguring these subtrees from the original layout, our approach requires only a few slight modifications \added[id=pan]{according to the minimum angle and the symmetry aesthetic metrics~\cite{DBLP:journals/vlc/Purchase02}} to tune the details (Figure~\ref{fig:price1000}d) because it generates an initial layout for each subtree. 

\added[id=pan]{
\textbf{Readability.} To evaluate the readability of the results generated by our approach, we use the measurements (crosslessness, minimum-angle metric, edge-length variation, and shape-based metric) in~\cite{DBLP:journals/tvcg/KwonCM18} to test readability improvement. All these measurements are normalized. Larger values of the measurements suggest higher readability except edge-length variation. Results of readability measurements for the Finan512 dataset, the Power-Network dataset and the Price\textunderscore1000 dataset are given in Figures~\ref{fig:Finan512}(h,i,j,k), Figures~\ref{fig:powernetwork}(e,f,g,h), Figures~\ref{fig:price1000}(e,f,g,h), accordingly. Bars representing measurement values before modification transfer are in orange and bars after modification transfer are in purple.
Note that, in the case study with the Price\textunderscore1000 dataset, we also measure the readability after slight modifications (in light purple). Results show that our approach improves readability in most cases.
}

\begin{figure*}[t!hbp]
    \centering
    \setlength{\belowcaptionskip}{-5pt}
    \includegraphics[width=2\columnwidth]{picture/price1000}
    \caption{The Price\textunderscore1000 dataset~\cite{DBLP:journals/cgf/KruigerRMKKT17}. (a) A radial tree layout. (b) A specified exemplar in which we specify two nodes with the two largest degrees as markers. This is modified into a radial tree layout interactively. (c) Targets and their counterparts after modification transfer. (d) Modified targets after several slight user modifications \added[id=pan]{(in red)}. \added{(e-h) Readability before (orange) and after (purple) modification transfer measured by four readability criteria (from top to bottom: crosslessness, minimum-angle metric, edge-length variant, and shape-based metric); error bars depict 95\% confidence intervals..}}
    \label{fig:price1000}
\end{figure*}


\subsection{User Study} \label{sec:userstudy}
We conducted a \replaced[id=pan]{within-participant}{between-subject} experiment in which we asked participants to fine-tune structures’ layouts in three modes:

\added{\textbf{1) Baseline manual}}: \deleted[id=pan]{manually} mouse dragging without our approach\deleted{(manual, $T_m$)};

\added{\textbf{2) Our semi-automatic method}}\deleted{: our approach} with markers specified by user\deleted{(semi-automatic, $T_s$)};

\added{\textbf{3) Our fully automatic method}} \deleted{: our approach} with markers initialized by filtering the results of FGMU\deleted{(fully automatic, $T_f$)}.

\textbf{\added[id=pan]{Task.}} Participants performed a task involving modifying the structure on the screen \replaced[id=pan]{according to the expert's modifications on the exemplar}{to the target structure made by visualization expert}. \added{Twenty substructures from four real-world datasets are used.}
%\added[id=pan]{designed in our study which focused on layout fine-tuning:}
%\begin{compactitem}[\textbf{T}1]
% \item 
% \added[id=pan]{}
%\end{compactitem}


\textbf{Datasets.} \replaced[id=pan]{A graph visualization expert helped us define the 20 total substructures used in the study. He first chose five exemplar substructures from four real-world datasets and then specified three target structures for each of the five exemplars (Figure~\ref{fig:usdata}). Graphs generated from four real-world datasets have already been laid out with FM$^3$~\cite{hachul2004drawing}. Substructures are extracted with node positions. The expert was also asked to modify five exemplars' layouts to support our task.
%One set of data consists of one exemplar (append with a re-layouted version modified by an expert which majored in graph visualization) and three target substructures. 
%We showed these 15 substructures 
%inherit from the origin layout of the real-world datasets which are computed by 
%using FM$^3$. See appendix for more details about these datasets.
}
{Four datasets were used in this study.}

\textbf{1)} The Email-Eu-core dataset~\cite{paranjape2017motifs} is a time-varying email contact network in a large European research institution with 986 nodes and 332,334 contacts. Email communications within every 24 hours form a graph, yielding a total of 803 snapshots with 855 connected subgraphs. \added[id=pan]{We obtained the first exemplar and its three target structures from Email-Eu-core dataset (Figure~\ref{fig:usdata}a). The expert modified the exemplar into a fan-like shape (Figure~\ref{fig:usdata}a-1).}

\textbf{2)} The Mouse-Brain dataset~\cite{DBLP:conf/miccai/FournierLE16} consists of 986 nodes and 1,536 edges. Nodes represent the mouse visual cortical neurons and edges are fiber tracts connecting one neuron to another. \added[id=pan]{We obtained the second exemplar and its three target structures from the Mouse-Brain dataset (Figure~\ref{fig:usdata}b). The expert modified the exemplar into a star-like shape in which the interior node stays in the center and the leaves are placed evenly around the interior (Figure~\ref{fig:usdata}b-1).}

\textbf{3)} The Euroroad dataset~\cite{vsubelj2011robust} is a road network mostly in Europe. Nodes represent cities and an edge between two nodes denote that they are connected. The network consists of 1,174 nodes and 1,417 edges. \added[id=pan]{We obtained the third exemplar and its three target structures (Figure~\ref{fig:usdata}c) are extracted from the Euroroad dataset~\cite{vsubelj2011robust}. The expert modified the exemplar into a round circle (Figure~\ref{fig:usdata}c-1).}

\textbf{4)} The High-School-contact dataset collected from the SocioPatterns initiative~\cite{10.1371/journal.pone.0136497} consists of 180 nodes and 45,047 contacts. We created a temporal network following the procedure in~\cite{von2009system}. 
\added[id=pan]{The last two exemplars and six target structures were obtained from the High-School-contact dataset (Figures~\ref{fig:usdata}(d, e)). The expert modified one exemplar into a shape in which the inner circle is laid out as a regular polygon and the surrounding nodes are placed orthogonally (Figure~\ref{fig:usdata}d-1). And he modified the other exemplar into an orthogonal layout (Figure~\ref{fig:usdata}e-1).}

    % \item 
    % We extracted two exemplars, on in orthogonal layout and the other rectangular layout
    % modified from the High-School-contact dataset, collected from the SocioPatterns initiative~\cite{10.1371/journal.pone.0136497}.  The nodes \changed{were}{should be placed} in a grid-like shape. Angles among links should be as close to 45$^{\circ}$, 90$^{\circ}$, or 180$^{\circ}$ as possible (Figure{~\ref{fig:usdata}} (e-1)).
    % Our last dataset is in circular shape  \replaced{The network}{The dataset} consists of 180 nodes and 45,047 \replaced{links}{contacts}. We create a temporal network following the procedure in.
    
    % The exemplar taken from $D_{HS1}$ contains a circle and some surrounding\deleted[id=pan]{s} nodes. The circle was modified into a regular polygon. And the links that form the circle and surroundings nodes should be placed horizontally or vertically (Figure~\ref{fig:usdata} (d-1)). 
    
    
    % \item The Visualization publications dataset~\cite{Isenberg:2017:VMC} contains the publications information of IEEE VIS from 1990 to 2018. For each year, the authors of one paper are connected to form a collaboration network. It yields to a dynamic network with 4,221 nodes, 10,535 links and 29 timestamps. The dynamic network consists of 573 connected components.
    
% The graphs generated from these datasets are layouted with a force-directed algorithm. 

\added[id=pan]{We ensured that within the same dataset, the Weisfeiler-Lehman similarities between three target substructures and the exemplar are greater than 0.7. We recorded all modifications made by the expert along with a list of instructions (see Suppl. Material).
}

\deleted[id=pan]{We extracted 5 exemplars from these datasets (one from the Mouse-Brain dataset, one from the Email-Eu-core dataset, two from the High-School-contact dataset, and one from the Euroroad dataset, respectively). For each dataset, we extracted three target substructures whose Weisfeiler-Lehman similarities with the exemplar are larger than $0.7$. One exemplar and three targets in one dataset formed a test case, yielding 5 cases: $D_E$ was from the Email-Eu-core dataset (Figure~\ref{fig:usdata}a), $D_M$ was from the Mouse-Brain dataset (Figure~\ref{fig:usdata}b), $D_R$ was from the Euroroad dataset (Figure~\ref{fig:usdata}c), and $D_{HS1}$ and $D_{HS2}$ were from the High-School-contact dataset (Figure~\ref{fig:usdata}(d, e)). $D_E$ was used in the tutorial and the others were used in the formal study.}

\begin{figure}[!tp]
    \centering
    \setlength{\belowcaptionskip}{-5pt}
    \includegraphics[width=1\columnwidth]{picture/usdata}
    \caption{\added[id=pan]{Data samples in the user study.} Five exemplars and 15 target substructures were extracted from the four datasets. (a) \added[id=pan]{is} extracted from the Email-Eu-core dataset~\cite{paranjape2017motifs}; 
    (b) \added[id=pan]{is from} the Mouse-Brain dataset~\cite{DBLP:conf/miccai/FournierLE16}; 
    (c) \replaced[id=pan]{is from the Euroroad dataset~\cite{vsubelj2011robust};}{ $D_{HS1}$ and (d) $D_{HS2}$ generated from the High-School-contact dataset;}
    \replaced[id=pan]{(d) and (e) are from the High-School-contact dataset~\cite{10.1371/journal.pone.0136497}.}{(e) $D_R$ taken from the Euroroad dataset; (f-j) exemplars reconfigured by an expert majored in graph visualization.}}
    \label{fig:usdata}
\end{figure}

\textbf{Participants and apparatus.}
Twelve volunteers were recruited to participate in the study (5 males, 7 females; aging from 23 to 27). All participants were students or researchers concentrating in computer science. They are familiar with visualization and four of them major in graph visualization. The study was conducted \replaced[id=pan]{on a PC provided by us}{on a personal computer} equipped with a mouse, keyboard, and 24-inch display. The interface was displayed within a window size of 1920 $\times$ 1080 resolution. \added[id=pan]{Parameters of the modification transfer are fixed to $\alpha=1, \beta=5, \gamma=1000, \text{ and } w=1$.}


\textbf{Study Conditions.}
We tested the performance of different fine-tuning techniques (\textbf{baseline manual}, \textbf{semi-automatic}, and \textbf{fully automatic}) on a small graph layout. Each participant was asked to \replaced[id=pan]{process three target structures in all four cases (one from the Mouse-Brain dataset, one from the Euroroad dataset, and two from the High-School-contact dataset) with three techniques}{process 4 cases (1 exemplar and 3 target structures in each case) with three techniques}, yielding \deleted[id=pan]{to} 432 ($12 \text{ participants} \times 4 \text{ cases} \times 3 \text{ targets} \times 3 \text{ techniques}$) trials.

\textbf{Procedure.} 
\replaced{The study has two stages. We first trained participants on the three manipulation modes (\textbf{baseline manual}, \textbf{semi-automatic}, and \textbf{fully automatic}). They viewed a demo video of an expert's operations using data samples extracted from the Email-Eu-core dataset (Figure~\ref{fig:usdata}a), and then practiced till they felt comfortable with the tasks. In the formal study, they were then asked to manipulate three targets' shapes to simulate the exemplar for each case using all three techniques ($4 \text{ cases } \times 3 \text{ targets } \times 3 \text{ techniques }$ in total for each participant).
For each trial, an exemplar, a modified exemplar, and a target substructure were displayed on the interface (see Suppl. Material). Participants were asked to manipulate the target substructure to simulate modifications made on the exemplar by comparing the exemplar and the modified exemplar. They could also follow printed instructions (see Suppl. Material).
With our semi-automatic method, participants were asked to specify markers first. 
One pair of markers was constructed by clicking on two nodes, one from the exemplar and one from the target substructure.
With our fully automatic method, markers were constructed automatically. Our two methods produced initial layouts that simulate the expert's modifications and participants were asked to perform the task based on initial layouts.
Parameters and initial layouts were the same for all participants.
The order of four cases, three techniques, and three targets was randomly assigned to each participant to counterbalance learning effects. After the study, participants were interviewed to give some suggestions on our approach.}
{Before the study, an expert majored in graph visualization was invited to modify the exemplar's shape according to his domain knowledge. We recorded his instructions and modifications:}

\deleted{The exemplar taken from $D_E$ was modified into a fan-like shape (Figure{~\ref{fig:usdata}} (f)).}

\deleted{The exemplar taken from $D_M$ is a star-like structure. The interior node should stay in the center of the exemplar and leaves should be evenly placed around the exemplar (Figure{~\ref{fig:usdata}} (g)).}

\deleted{The exemplar taken from $D_R$ is a circle-like structure. Its shape should be as circular as possible. (Figure{~\ref{fig:usdata}} (h)).}

\deleted{The exemplar taken from $D_{HS1}$ was modified into an orthogonal layout. The nodes should be placed in a grid-like shape. Angles among edges should be as close to 45$^{\circ}$, 90$^{\circ}$, or 180$^{\circ}$ as possible (Figure{~\ref{fig:usdata}} (i)).}

\deleted{The exemplar taken from $D_{HS2}$ contains a circle and some surrounding\deleted[id=pan]{s} nodes. The circle was modified into a regular polygon. And the edges that form the circle and surroundings nodes should be placed horizontally or vertically (Figure~\ref{fig:usdata}j).
}

\deleted{At the beginning of the study, we introduce our approach to participants and show them a tutorial with $D_E$. Then, for each case, three techniques are employed randomly. Participants need to manipulate three targets' shapes to simulate the exemplar according to the instructions given by the expert. % The layouts of the exemplar before and after modifying are shown.
%  by side. After three targets are fine-tuned within one technique, participants can take a break. 
We counterbalance the techniques in different participants and datasets.}

% \todo{JC: I think we would need to be clear about # of cases each participant did, are they the same or just randomly chosen. How many cases are prepared?}

\textbf{Hypotheses.}
We measure performance by participants' completion time and number of interactions. We anticipate that the quality of the modified exemplar and the targets' layouts makes little difference because participants were asked to fine-tune the target layouts until they were satisfied.
We formulated three hypotheses:
\begin{compactenum}[\bfseries H1]
\item Our fully automatic method is more efficient than the baseline manual method.

\item Our semi-automatic method is more efficient than the baseline manual method.

\item There is no difference in performance between our semi-automatic method and our fully automatic method.
% 
\end{compactenum}

\textbf{Results.}
\added[id=pan]{Participants spent about 45 minutes on average on the user study and got a reward of around \$5 on completion.}
We recorded the number of interactions (\added[id=pan]{mouse clicking and dragging}) that participants performed and completion times to reach a satisfying layout. The completion time includes marker specification, algorithm computation, and layout modification; and the number of interactions includes marker specification and layout modification. Figures~\ref{fig:usresultnew}(a, b) summarizes the results. 
We analyzed our results using significance tests with significance levels set to $.05$.


\added[id=pan]{The Shapiro-Wilk test, used to test the normality, suggested that both the number of interactions and the completion time did not follow normal distributions. Thus we used the Friedman test and pairwise Wilcoxon test. The Friedman test detected significant differences in both the number of interactions ($\chi^2(2)=154.96, p<0.05$) and the completion time ($\chi^2(2)=154.625, p<0.05$). Paired Wilcoxon tests were performed on all cases to compare the efficiency among three techniques. There were significant differences among all combinations of three techniques (\textbf{baseline manual}, \textbf{semi-automatic}, and \textbf{fully automatic}) on two measurements (the number of interactions and the completion time). The post-hoc analysis (Figures~\ref{fig:usresultnew}(a, b)) showed that our semi-automatic method performed most efficiently in both two measurements, followed by the baseline method and last our semi-automatic method. Thus \textbf{H1} held while \textbf{H2} and \textbf{H3} were rejected.}

\deleted{
\textbf{The number of interactions.} The Shapiro-Wilk test is employed to test the normality of the numbers of interactions. The results suggest that the numbers of interactions do not follow a normal distribution. Thus we use the Friedman test and pairwise Wilcoxon test. The Friedman test detects significant differences ($\chi^2(2)=103.10, p<0.05$). Paired Wilcoxon tests are performed on all cases to compare the efficiency among three techniques. There are significant differences among all different combinations of three techniques ($T_f$ (fully-automatic), $T_s$ (semi-automatic) and $T_m$ (manual)). The post-hoc analysis shows that $T_f$ performs most efficiently, followed by $T_s$ and $T_m$. Thus, \textbf{H1} and \textbf{H2} hold while \textbf{H3} is rejected.}

\deleted{\textbf{Completion time.} The Shapiro-Wilk test suggests that the completion time does not follow a normal distribution. Thus we use the Friedman test to test differences among three techniques. It detects significant differences among three techniques. Significant differences are detected by paired Wilcoxon tests among all different combinations of three techniques. Figure~\ref{fig:usresult} (b) shows that $T_f$ is the most efficient technique. And $T_s$ performs slightly better than $T_m$. As such, \textbf{H3} is rejected while \textbf{H1} and \textbf{H2} hold.}

\deleted{Overall, the results indicate our approaches $T_f$ (fully-automatic) and $T_s$ (semi-automatic) perform better than manually reconfiguration, which fully supports \textbf{H1} and \textbf{H2}, but mostly contradicts \textbf{H3}.}


\textbf{\added[id=pan]{Feedback.}}
\added[id=pan]{We collected some representative participant feedback. Most of them made comments along the lines of, ``\textit{In fully automatic mode, most results are pretty close to exemplar's results. I have to make little effort to modify them, especially in complex cases. But I still have to verify whether there is room for improvement}".
Many of them mentioned that they were encouraged to attempt higher quality by the high-quality result generated by the fully automatic method. Some of them mentioned that ``\textit{It is boring to wait for the fully automatic method to calculate the result}". Another complaint about our methods is that markers are hard to determine. Most participants had little experience in graph visualization. Interestingly, several participants mentioned that ``\textit{The user study is like a game, fine-tuning layouts makes me feel relaxed because I generate nice-looking results}". One of them suggested expanding our user study into an online system to collect more user data.
% 1. 类似一个toy，非常有意思，建议我们未来能公开，以收集更多数据；
% 2. 非常难以选择marker，半自动模式对于非图可视化的可视化从业者非常不友好；
% 3. 对自动化算法评价很高，虽然有些时候对结果已经很满意了，但是他们仍然会微调一下，是因为在高质量的结果下，他们被鼓励向更高质量的结果努力。
% 4. 手动方法虽然比较慢，但也充满了乐趣，将一些不好的布局调整成为规则的布局使他们心情愉快。
% 5. 自动化算法有一个较长的等待时间，在此期间他们会感到一些无聊。
}

\begin{figure}[t!p]
    \centering
    \setlength{\belowcaptionskip}{-5pt}
    \includegraphics[width=1\columnwidth]{picture/usresultstacked}
    \caption{User study results. \added[id=pan]{Measurement components are represented as stacked bars.} (a) The distribution of the number of interactions; (b) the distribution of completion time; (c) the distribution of number of interactions on different cases; (d) the distribution of completion time on different cases. Error bars depict 95\% CIs.}
    \label{fig:usresultnew}
\end{figure}

\textbf{Discussion.}
\replaced[id=pan]{
We split the number of interactions and the completion time to look for deeper insights. The completion time consists of three parts: marker specification, algorithm computation, and interactive layout modification (Figure~\ref{fig:usresultnew}d). The computation time occupies a small fraction (in green) in both the semi-automatic and fully automatic methods. The marker specification (in orange) contributes a lot to the completion time of our semi-automatic method. In most cases, participants spent most time on interactively modifying layouts.
% To investigate the principal factors influencing completion time in our two methods, we split the completion time into three parts (marker specification, algorithm computation, and interactive layout modification) to look for deeper insights (Figure~\ref{fig:usresultnew}d).
% Algorithm computation time only occupies a fraction of the completion time (stacked green bars in Figure~\ref{fig:usresultnew}d).
% We find marker specification (stacked orange bars) is important in making our semi-automatic method works poorly.
% In most cases, participants mainly spent their time on interactively modifying layouts.
We also calculated average completion time per interaction for the three methods; participants spent an average of 2.4 second, 2.6 second, and 3.3 second on each layout modifying interaction using the baseline method, our semi-automatic method, and our fully automatic method, respectively.
Participants spent more time thinking about and verifying results generated by our fully automatic method.
Each interaction for marker specification takes an average of 4.1 seconds. 
We observe that almost all participants tended to choose internal nodes in the star-like structures (Figure~\ref{fig:usdata}b) as markers. However, for structures extracted from the High-School-contact dataset, markers were diverse among participants.
We report results of specified markers by an expert on graph analysis in the Suppl. Material.
% 一对好的marker应该能够在source和target中承担相同的角色或地位（比如割点）。
A good pair of markers should be able to assume the same role or status in the source and the target (e.g., cut nodes).
This indicates that experience in and knowledge of graph analysis are necessary for marker specifications.
% And each interaction for markers specification took an average of 4.1 second.
% Participants needed time to determine markers.
% \highlight{We invited the expert to specify several markers to study which kind of markers can generate good results. Results generated with expert specified markers can be found in the supplemental material.}
% It seems that a good pair of markers should play the same role in graphs they belong to (e.g. cut points). 
% Thus, our semi-automatic method is not recommended for users with little experience in graph theory and graph visualization, because the marker specification step requires domain knowledge.
}
{The user study verifies the efficiency of our approach. Details of results of our user study are shown in Figure~\ref{fig:usresult} (c) and (d). Our approach supports specifying markers according to their domain knowledge. The user study suggests that participants perform diversely when markers are specified by themselves (semi-automatic, $T_s$). Error bars show that $T_s$ performs more efficiently than $T_m$ for $D_M$ and $D_R$ while $T_m$ is slightly better for $D_{HS1}$ and $D_{HS2}$. The reason may be that topology structures of structures in $D_{HS1}$ and $D_{HS2}$ are more complex. Participants with little experience in graph visualization can not easily select markers.
While for $D_M$ and $D_R$ whose topological structures are more simple, $T_s$ performs better than $T_m$. Markers are more easily to be determined. Another finding is that the average completion time per interaction differs in different techniques. The average time of $T_f$ is much larger than those of others.
The reason may be that completion time of $T_f$ is mainly spent on computing correspondences. Note that, our approach employs factorized graph matching (FGM) is employed in our approach, whose time complexity for matching $S=(V^s, E^s)$ and $T=(V^t, E^t)$ is $O(k((|V^s| + |E^s|)(|V^t| + |E^t|)) + max(|V^s|^3 + |E^s|^3))$, where $k$ is the number of iterations in FGM. 
}

% 下面部分是对时间复杂度的描述
% \subsection{\added[id=pan]{Performance of Modification Transfer}} \label{sec:performance}
% \highlight{
% \added[id=pan]{We tested the performance of modification transfer algorithm with synthetic datasets consists of several random graphs with different scales. First, we employed a random graph generator~\cite{gilbert1959random} to generate random graphs. Two parameters are involved in the generator: $n$ is the number of nodes and $p$ is the link creation probability. We generated 15 graphs with $n = 20, 40, 60, 80,\text{ and }100$ and $p = 0.3, 0.5, \text{ and }0.7$. We repeated it again to generate another 15 graphs to construct 15 graph pairs with the first 15 graphs. For each pair of graphs (denoted as $S$ and $T$), we layouted them with FM$^3$ and re-layouted $S$ into $S^{\prime}$ alone. We transferred modifications between $S$ and $S^{\prime}$ to $T$ and recorded how long did different parts of process run and how much iterations were performed in Figure~\ref{fig:performance}. 
% We fixed the weighting parameters to $\alpha=1, \beta=5, \gamma=1000,\text{ and }w=1$. The procedure above was repeated five times to get an average result.

% The graph matching procedure is time-consuming. We utilize FGMU~\cite{zhou2012factorized} to generate correspondences. Its time complexity for matching $S=(V^s, E^s)$ and $T=(V^t, E^t)$ can be larger than $O(k \times \max(|V^t|^3, |V^s|^3) + |E^t||E^s|^2))$, where $k$ is the number of iteration. Thus, Figure~\ref{fig:performance}? shows that the computation time of FGMU increases dramatically with the increase of nodes and links. However, the procedure to produce correspondence in our approach is extensible. Its time complexity depends on which graph matching algorithm is chosen. Filtering correspondence costs less in computation which can be estimated to $O(\min(|V^t|, |V^s|) \times \frac{|E^t|}{|V^t|} \times \frac{|E^s|}{|V^s|} )$. If markers are specified by the user, graph matching can be eliminated. The performance of \textbf{the layout simulation}
% }}